{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN1tZ4BoHFKn"
      },
      "source": [
        "# **Homework 2: Image segmentation without user input**\n",
        "\n",
        "## Overview\n",
        "This homework has two parts.\n",
        "\n",
        "The first part of this homework is to implement [Grad-CAM](https://arxiv.org/pdf/1610.02391) â€“ a method for producing saliency maps (heatmaps of regions that are most relevant to a model) that uses both activations and gradients of the feature maps of a convolutional layer. Feature maps of deeper convolutional layers represent more high-level features, while preserving rough spatial structure, which makes them a good candidate for explaining a model's output.\n",
        "\n",
        "The second part of this homework is to use SAM for image segmentation without user input.\n",
        "SAM [(Segment Anything Model v1)](https://arxiv.org/pdf/2304.02643) is a popular family of open-weight models for image segmentation (based on the vision transformer ViT and CLIP).\n",
        "The model takes as input an image to be segmented and additionaly bounding boxes, point coordinates etc. clarifying the object of interest to be segmented. It can output many proposed segmentations of many objects on one image. Your task will be to find appropriate point coordinates automatically, so that SAM can be used with just an image input to segment particular objects.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset\n",
        "\n",
        "You will use a small custom dataset based on [CIFAR10](https://en.wikipedia.org/wiki/CIFAR-10), but containing images with one of five objects (circle, square, diamond, triangle, star), with ground-truth segmentations of that object.\n",
        "\n",
        "---\n",
        "\n",
        "## GradCAM task\n",
        "\n",
        "For the first task:\n",
        "* Read the original [Grad-CAM](https://arxiv.org/pdf/1610.02391) paper.\n",
        "* Implement it, without using non-standard packages (the only imports allowed are built-ins, torch, torchvision, numpy, scipy, and helpers like cv2, PIL, tqdm, matplotlib).\n",
        "* The result should be a class named `GradCAM` with methods:\n",
        "    * `def __init__(model: nn.Module, target_layers=Iterable[nn.Module])`\n",
        "    * `def __call__(self, image: Tensor, targets: Iterable[int] | None = None) -> np.ndarray` where\n",
        "        * `image` is an input to `model` (a normalized batch of shape `B,C,H,W`).\n",
        "        * `targets` is an iterable of target classes that we want to segment; if None is given, use the top class predicted by the model.\n",
        "        * The result is a numpy array of shape (B, H, W) containing the GradCam heatmap, with `min..max` values rescaled to `0..1` (independently for each image in the batch and each `target_layers`). If more than one `target_layers` was given, return the average of the resulting heatmaps.      \n",
        "    * Feel free to add optional/default arguments and additional methods.\n",
        "* Check your implementation by running the code under the `GradCAM results` header.\n",
        "\n",
        "Tip: you may find it particularly useful to use: [nn.Module.register_full_backward_hook](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook).\n",
        "\n",
        "\n",
        "## Segmentation task\n",
        "\n",
        "For the second part, SAM is given as a black-box. You must design a pipeline that takes only an image and outputs a segmentation of the circle/square/diamond/triangle/star shape. The input to SAM should be the image and coordinates of point(s) that are likely to be inside (\"foreground\") or likely to be outside (\"background\") of the shape. **The coordinates must be the output of the previous step of the pipeline. They cannot be provided directly by user.**\n",
        "\n",
        "* Think of a way to find appropriate points. Try two different approaches:\n",
        "    1. at least one foreground point, without background points.\n",
        "    2. at least one foreground point and at least one background point.\n",
        "* Implement both approches as subclasses of `BasicSamPipeline`, overriding the `__call__` method (preserving the signature).\n",
        "* Evaluate your generated point(s) and report the following metrics:\n",
        "    * *hit rate*: how often they fall inside the ground-truth mask;\n",
        "    * *distance*: distance from the center of mass of the ground-truth mask\n",
        "        (the average coordinate of True pixels in the mask).\n",
        "* Evaluate your overall pipeline and report the following metric:\n",
        "    * *Intersection over Union (IoU)* of the predicted and ground-truth masks, averaged over all images in the dataset.\n",
        "\n",
        "\n",
        "**Important**: This task is not about finding the pipeline with best hyperparameters; we expect an IoU of at least `65%`, but achieving results higher than that will not affect the grade for the assignment.\n",
        "\n",
        "**Important**: Do not train or fine-tune your own models, only use the ones provided (the classifier and SAM).\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "On [Moodle](https://moodle.mimuw.edu.pl/mod/assign/view.php?id=159965), submit a .zip archive with:\n",
        "\n",
        "1. **Notebook** (`.ipynb`):\n",
        "    * It should contain:\n",
        "        * The GradCAM implementation.\n",
        "        * The two `BasicSamPipeline` subclasses.\n",
        "        * Evaluations with computations of all reported metrics.\n",
        "    * It should be possible to execute the whole notebook start-to-end without human intervention.\n",
        "    * Such an execution should take less than 30 minutes on a Colab GPU.\n",
        "    * Do not modify (but *do* execute) cells under headers marked as `[do not modify]`. If you wish to extend them (e.g. to check more GradCAM results), you may do so **under a new header**.\n",
        "\n",
        "2. **Report (1-2 pages, PDF)** including:\n",
        "   * An examplary visualization of the output of the Grad-CAM\n",
        "   * A concise description of each approach for the SAM pipelines (1-3 sentences each).\n",
        "   * A presentation of all metrics.\n",
        "   * Discussion (up to 5 sentences) on potential areas for improvements.\n",
        "\n",
        "3. **README.md**:\n",
        "   * Link to Colab version of the notebook for fast replication.\n",
        "\n",
        "\n",
        "\n",
        "## Grading\n",
        "\n",
        "1. Implementation correctness of Grad-CAM:  30%\n",
        "2. Implementation correctness of the multistage pipeline and evaluations: 50%\n",
        "3. Report & analysis: 20%\n",
        "\n",
        "Please take care of readability, clear structure in particular (headers in notebooks, modular code).\n",
        "This will be considered within each grading component.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhWEaao9W2SF"
      },
      "source": [
        "# 0. Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMwHjowPDYBj"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEZpNKdv1-Fo",
        "outputId": "1be07483-22a8-4709-fe94-296ed6188e24"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision matplotlib opencv-python-headless numpy segment-anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53n0Mpm36Sou"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from collections.abc import Callable, Iterable\n",
        "from pathlib import Path\n",
        "from typing import Any, Final, Literal, TypedDict\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import scipy.ndimage\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models\n",
        "from torch import Tensor, nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import v2\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiAHPuz5W2SG"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXa0LsWEW2SI"
      },
      "source": [
        "## Dataset [do not modify]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayIU_CAawBTZ"
      },
      "source": [
        "CIFAR-10 download takes 170 MiB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdQ_0VKiW2SI"
      },
      "outputs": [],
      "source": [
        "%%bash --no-raise-error\n",
        "mkdir -p data/\n",
        "wget -nc -q -O data/synthetic_shapes.zip https://www.mimuw.edu.pl/~mwrochna/upload/synthetic_shapes.zip\n",
        "unzip -d data/ data/synthetic_shapes.zip &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKr8TlwTW2SI"
      },
      "outputs": [],
      "source": [
        "class SyntheticData[T](Dataset):\n",
        "    \"\"\"A small synthetic segmentation dataset.\n",
        "\n",
        "    It is a sequence dataset of 5000 tuples (image, class, mask), where:\n",
        "    - image: before transformation, an RGB PIL Image.\n",
        "    - class: int 0..4, the label index.\n",
        "    - mask:  numpy array of dtype=bool, shape (H, W), same size as image.\n",
        "    \"\"\"\n",
        "\n",
        "    CLASSES: Final[tuple[str, ...]] = (\"circle\", \"square\", \"triangle\", \"star\", \"diamond\")\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path: Path,\n",
        "        split: Literal[\"train\", \"val\", \"test\"],\n",
        "        transform: Callable[[PIL.Image.Image], T],\n",
        "    ) -> None:\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "\n",
        "        self.class_to_idx = {class_name: i for i, class_name in enumerate(self.CLASSES)}\n",
        "\n",
        "        images = sorted((self.path / \"images\" / self.split).glob(\"*.png\"))\n",
        "        masks = sorted((self.path / \"masks\" / self.split).glob(\"*.png\"))\n",
        "        labels = sorted((self.path / \"labels\" / self.split).glob(\"*.txt\"))\n",
        "\n",
        "        assert images, f\"No images found in {self.path / 'images' / self.split}\"\n",
        "        assert len(images) == len(masks) == len(labels), (\n",
        "            \"Number of images, masks, and labels must be the same\"\n",
        "        )\n",
        "        assert [p.stem for p in images] == [p.stem for p in masks] == [p.stem for p in labels], (\n",
        "            \"Image/mask/label filename mismatch.\"\n",
        "        )\n",
        "        self.image_names = [p.stem for p in images]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx) -> tuple[T, int, np.ndarray]:\n",
        "        name = self.image_names[idx]\n",
        "        img_path = self.path / \"images\" / self.split / f\"{name}.png\"\n",
        "        mask_path = self.path / \"masks\" / self.split / f\"{name}.png\"\n",
        "        label_path = self.path / \"labels\" / self.split / f\"{name}.txt\"\n",
        "\n",
        "        img = PIL.Image.open(img_path).convert(\"RGB\")\n",
        "        mask = np.array(PIL.Image.open(mask_path), dtype=bool)\n",
        "        label = self.class_to_idx[label_path.read_text().strip()]\n",
        "\n",
        "        img_transformed: T = self.transform(img)\n",
        "\n",
        "        return img_transformed, label, mask\n",
        "\n",
        "\n",
        "def show_image_row(\n",
        "    image_dict: dict[str, PIL.Image.Image | np.ndarray | Tensor], size: float = 3.0\n",
        ") -> None:\n",
        "    n = len(image_dict)\n",
        "    _, axs = plt.subplots(1, n, figsize=(size * n, size), constrained_layout=True, squeeze=True)\n",
        "    if n == 1:\n",
        "        axs = [axs]\n",
        "    for ax, (title, img) in zip(axs, image_dict.items(), strict=True):\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(title)\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "u0brJlsbW2SI",
        "outputId": "d0f4a91d-fbd6-43a4-93e9-3715060346cb"
      },
      "outputs": [],
      "source": [
        "dataset_path = Path(\"./data/synthetic_shapes\")\n",
        "\n",
        "\n",
        "def example_from_dataset(idx: int = 3):\n",
        "    for split in (\"train\", \"val\", \"test\"):\n",
        "        dataset = SyntheticData(dataset_path, split=split, transform=lambda x: x)\n",
        "        print(f\"{split} dataset size: {len(dataset)}\")\n",
        "\n",
        "    img, label, mask = dataset[idx]\n",
        "    show_image_row({\"Image\": img, \"Mask\": mask})\n",
        "    print(f\"Label: {label} ({SyntheticData.CLASSES[label]})\")\n",
        "\n",
        "\n",
        "example_from_dataset(3)\n",
        "example_from_dataset(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZPb1h56W2SI"
      },
      "source": [
        "## Training a classifier [do not modify]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eAbbyg07J_6"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        "    epochs: int = 5,\n",
        "    **optimizer_kwargs: Any,\n",
        ") -> None:\n",
        "    print(f\"ðŸš€ Training CNN for {epochs} epochs...\")\n",
        "    optimizer = optim.AdamW(model.parameters(), **optimizer_kwargs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        _train_epoch(model, train_loader, optimizer, desc=f\"Epoch {epoch + 1}/{epochs} training  \")\n",
        "        val_metrics = evaluate(model, val_loader, desc=f\"Epoch {epoch + 1}/{epochs} validation\")\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}/{epochs} â€” \"\n",
        "            + f\"val loss: {val_metrics['loss']:.3f}, val acc: {val_metrics['accuracy']:.1%}\"\n",
        "        )\n",
        "\n",
        "    test_metrics = evaluate(model, test_loader, desc=\"Test Evaluation\")\n",
        "    print(\n",
        "        \"âœ… Model training complete: \"\n",
        "        + f\"Test loss: {test_metrics['loss']:.3f}, test acc: {test_metrics['accuracy']:.1%}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def _train_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    desc: str,\n",
        ") -> dict[str, float]:\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    progress_bar = tqdm(dataloader, desc=desc)\n",
        "    for imgs, labels, _ in progress_bar:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, dim=1)\n",
        "        total_samples += labels.shape[0]\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix(\n",
        "            train_loss=f\"{total_loss / (total_samples / labels.shape[0]):.3f}\",\n",
        "            train_acc=f\"{total_correct / total_samples:.1%}\",\n",
        "        )\n",
        "\n",
        "    return {\"loss\": total_loss / len(dataloader), \"accuracy\": total_correct / total_samples}\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module, dataloader: torch.utils.data.DataLoader, desc: str\n",
        ") -> dict[str, float]:\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, _mask in tqdm(dataloader, desc=desc):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, dim=1)\n",
        "            total_samples += labels.shape[0]\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return {\"loss\": total_loss / len(dataloader), \"accuracy\": total_correct / total_samples}\n",
        "\n",
        "\n",
        "class DataloaderArgs(TypedDict, total=False):\n",
        "    batch_size: int\n",
        "    shuffle: bool\n",
        "    num_workers: int\n",
        "    pin_memory: bool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aibqp-L-W2SJ",
        "outputId": "80f0c64a-179c-4be7-ec15-9f4e04e93026"
      },
      "outputs": [],
      "source": [
        "device = torch.accelerator.current_accelerator(check_available=True) or torch.device(\"cpu\")\n",
        "use_accel = device != torch.device(\"cpu\")\n",
        "print(use_accel, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfU07l1CW2SJ",
        "outputId": "c59d7e31-2927-40ce-f6fd-ad34dbe0ea73"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "transform = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "    ]\n",
        ")\n",
        "inverse_transform = v2.Compose(\n",
        "    [\n",
        "        v2.Normalize(\n",
        "            [-m / s for m, s in zip(IMAGENET_MEAN, IMAGENET_STD, strict=True)],\n",
        "            [1 / s for s in IMAGENET_STD],\n",
        "        ),\n",
        "        v2.ToPILImage(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = SyntheticData(dataset_path, transform=transform, split=\"train\")\n",
        "val_dataset = SyntheticData(dataset_path, transform=transform, split=\"val\")\n",
        "test_dataset = SyntheticData(dataset_path, transform=transform, split=\"test\")\n",
        "\n",
        "train_kwargs: DataloaderArgs = {\n",
        "    \"batch_size\": 128,\n",
        "    \"num_workers\": 2,\n",
        "    \"shuffle\": True,\n",
        "    \"pin_memory\": use_accel,\n",
        "}\n",
        "val_kwargs: DataloaderArgs = {\"batch_size\": 500, \"num_workers\": 2, \"pin_memory\": use_accel}\n",
        "test_kwargs: DataloaderArgs = val_kwargs\n",
        "\n",
        "train_loader = DataLoader(train_dataset, **train_kwargs)\n",
        "val_loader = DataLoader(val_dataset, **val_kwargs)\n",
        "test_loader = DataLoader(test_dataset, **test_kwargs)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"âœ… DataLoaders created for train, validation, and test sets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45_h0YFAd45L",
        "outputId": "1f4037f6-320b-4fe7-f559-f5bfa5fae075"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = Path(\"./model_checkpoint.pth\")\n",
        "\n",
        "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(SyntheticData.CLASSES))\n",
        "model = model.to(device)\n",
        "\n",
        "if not checkpoint_path.exists():\n",
        "    train(model, train_loader, val_loader, test_loader, epochs=5, lr=2e-3, weight_decay=0.05)\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "else:\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device, weights_only=True))\n",
        "    metrics = evaluate(model, test_loader, desc=\"Test Evaluation\")\n",
        "    print()\n",
        "    print(\n",
        "        \"âœ… Model loaded from checkpoint: \"\n",
        "        + f\"Test loss: {metrics['loss']:.3f}, test acc: {metrics['accuracy']:.1%}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dNdj7aZvIaG"
      },
      "source": [
        "# 1. GradCAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjORc8RAJLuX"
      },
      "source": [
        "## GradCAM implementation (add your code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5SHjz6k6vRf"
      },
      "outputs": [],
      "source": [
        "class GradCAM:\n",
        "    \"\"\"\n",
        "    Pure PyTorch implementation of Grad-CAM (Gradient-weighted Class Activation Mapping).\n",
        "\n",
        "    Grad-CAM highlights the important regions in the input that contribute most to\n",
        "    the model's prediction for a given target class. It uses both activations and\n",
        "    gradients of the feature maps from a convolutional layer.\n",
        "\n",
        "    Usage:\n",
        "        grad_cam = GradCAM(model=model, target_layers=[layer1, layer2])\n",
        "        grayscale_cam = grad_cam(input_tensor, targets=[class_id])\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network to compute Grad-CAM for.\n",
        "        target_layers (Iterable[nn.Module]): The convolutional layers to target\n",
        "            for Grad-CAM computation. Feature maps and their gradients from these\n",
        "            layers will be used.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, target_layers: Iterable[nn.Module]) -> None:\n",
        "        self.model = model\n",
        "        self.target_layers = target_layers\n",
        "        # List to store gradients during backward pass, though not directly used in final CAM computation here,\n",
        "        # hooks handle the access to gradients directly from the layers.\n",
        "        self.gradients = []\n",
        "\n",
        "    def __call__(self, input_tensor: Tensor, targets: Iterable[int] | None = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes Grad-CAM heatmaps for the input tensor.\n",
        "\n",
        "        Args:\n",
        "            input_tensor (Tensor): Input tensor of shape (B, C, H, W) or (C, H, W).\n",
        "                                   It will be unsqueezed if it's a single image.\n",
        "            targets (Iterable[int] | None): List of target class indices for which to compute\n",
        "                Grad-CAM. If None, the class with the highest predicted score will be used.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Grad-CAM heatmaps of shape (B, H, W). Values are normalized to the range [0, 1].\n",
        "                        If multiple `target_layers` are provided, the heatmaps are averaged.\n",
        "        \"\"\"\n",
        "\n",
        "        # Add batch dimension if input is a single image\n",
        "        if input_tensor.dim() == 3:\n",
        "            input_tensor = input_tensor.unsqueeze(0)\n",
        "\n",
        "        # Ensure that input_tensor requires gradients for backpropagation\n",
        "        # This is crucial for Grad-CAM to work as it relies on gradients w.r.t. feature maps\n",
        "        input_tensor.requires_grad_(True)\n",
        "\n",
        "        B, C, H, W = input_tensor.shape\n",
        "        # Placeholder for collected heatmaps from different layers\n",
        "        layers_heatmaps = []\n",
        "\n",
        "        # Define hooks to capture activations and gradients\n",
        "        # The forward hook captures the output feature maps (activations)\n",
        "        def save_output_hook(module, input, output):\n",
        "            module.saved_output = output.detach()  # Detach to prevent modifying the computational graph\n",
        "\n",
        "        # The backward hook captures the gradients flowing back to the feature maps\n",
        "        def save_grad_hook(module, grad_input, grad_output):\n",
        "            # grad_output[0] contains the gradient w.r.t. the output of the module\n",
        "            module.saved_grad = grad_output[0]\n",
        "\n",
        "        handles = []\n",
        "        try:\n",
        "            # Register hooks for each target layer\n",
        "            for layer in self.target_layers:\n",
        "                handles.append(layer.register_forward_hook(save_output_hook))\n",
        "                handles.append(layer.register_backward_hook(save_grad_hook))\n",
        "\n",
        "            # Set model to evaluation mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Forward pass: compute model output\n",
        "            output = self.model(input_tensor)\n",
        "\n",
        "            # Determine the target class(es) for which to compute Grad-CAM\n",
        "            if targets is None:\n",
        "                # If no specific target is given, use the class with the highest predicted score\n",
        "                targets = output.argmax(dim=1)\n",
        "\n",
        "            # Zero gradients before backward pass\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            # Compute the sum of scores for the target classes\n",
        "            # This sum becomes the 'loss' for backpropagation to calculate gradients w.r.t. target classes\n",
        "            loss = output[:, targets].sum()\n",
        "\n",
        "            # Backward pass: compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Iterate through target layers to compute Grad-CAM\n",
        "            for layer in self.target_layers:\n",
        "                # Retrieve saved activations and gradients from the hooks\n",
        "                activations = layer.saved_output  # A^k in the Grad-CAM paper\n",
        "                gradients = layer.saved_grad      # âˆ‚Y^c/âˆ‚A^k in the Grad-CAM paper\n",
        "\n",
        "                # Compute the neuron importance weights (alpha_k)\n",
        "                # Global average pooling of gradients: Î±_k^c = (1/Z) Î£_i Î£_j âˆ‚Y^c / âˆ‚A_{ij}^k\n",
        "                weights = gradients.mean(dim=(2, 3), keepdim=True)  # Shape: (B, C_k, 1, 1)\n",
        "\n",
        "                # Compute the weighted activation map (L_Grad-CAM^c)\n",
        "                # Sum of (weights * activations) across channels, then ReLU\n",
        "                # L_Grad-CAM^c = ReLU(Î£_k Î±_k^c A^k)\n",
        "                cam = F.relu((weights * activations).sum(dim=1, keepdim=True))\n",
        "\n",
        "                # Upsample the CAM to the original image size\n",
        "                cam = F.interpolate(cam, size=(H, W), mode='bilinear', align_corners=False)\n",
        "\n",
        "                # Normalize the heatmap to the range [0, 1] for visualization\n",
        "                cam = cam.squeeze(1).detach().cpu().numpy()  # Convert to numpy array (B, H, W)\n",
        "                # Normalize independently for each image in the batch\n",
        "                cam_min = cam.min(axis=(1, 2), keepdims=True)\n",
        "                cam_max = cam.max(axis=(1, 2), keepdims=True)\n",
        "                cam = (cam - cam_min) / (cam_max - cam_min + 1e-8) # Add epsilon to prevent division by zero\n",
        "                layers_heatmaps.append(cam)\n",
        "\n",
        "            # Average heatmaps from all target layers if multiple were specified\n",
        "            final_heatmaps = np.mean(np.array(layers_heatmaps), axis=0)\n",
        "\n",
        "        finally:\n",
        "            # Always remove hooks to prevent memory leaks and unintended behavior\n",
        "            for handle in handles:\n",
        "                handle.remove()\n",
        "\n",
        "        # Clear model gradients after computation\n",
        "        self.model.zero_grad()\n",
        "        return final_heatmaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh--E5fkKfPi"
      },
      "source": [
        "## GradCAM results [do not modify]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPuP7qEwKfPi"
      },
      "outputs": [],
      "source": [
        "if type(model).__name__ == \"ResNet\":\n",
        "    target_layers = [model.layer2[-1]]\n",
        "else:\n",
        "    target_layers = [\n",
        "        model.get_submodule(\"features.2.0\"),\n",
        "        model.get_submodule(\"features.3.0\"),\n",
        "        model.get_submodule(\"features.4.0\"),\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXkoUH5jKfPj"
      },
      "outputs": [],
      "source": [
        "def heatmap_to_rgb_image(\n",
        "    heatmap: np.ndarray, min: float | None = None, max: float | None = None\n",
        ") -> PIL.Image.Image:\n",
        "    \"\"\"\n",
        "    Converts a single-channel heatmap to an RGB pillow image using a colormap.\n",
        "\n",
        "    Args:\n",
        "    - heatmap: shape (H, W), will be normalized by mapping min..max to 0..1.\n",
        "    - min: minimum value for normalization, defaults to heatmap.min().\n",
        "    - max: maximum value for normalization, defaults to heatmap.max()\n",
        "    \"\"\"\n",
        "    heatmap = heatmap.astype(np.float32)\n",
        "    if min is None:\n",
        "        min = heatmap.min()\n",
        "    if max is None:\n",
        "        max = heatmap.max()\n",
        "    heatmap = (heatmap - min) / (max - min + 1e-8)\n",
        "    heatmap_uint8 = (np.clip(heatmap, 0.0, 1.0) * 255).astype(np.uint8)\n",
        "    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
        "    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
        "    return PIL.Image.fromarray(heatmap_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "VryRBMa2KfPj",
        "outputId": "70b6f884-285d-4089-cdd2-d3731bfefd94"
      },
      "outputs": [],
      "source": [
        "def example_gradcam():\n",
        "    grad_cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "    for test_idx in [3, 10, 42]:\n",
        "        img, label, mask = test_dataset[test_idx]\n",
        "\n",
        "        cam = grad_cam(img.unsqueeze(0).to(device), targets=[label])\n",
        "        heatmap_img = heatmap_to_rgb_image(cam.squeeze(0), 0, 1)\n",
        "\n",
        "        show_image_row(\n",
        "            {\n",
        "                \"Input image\": inverse_transform(img),\n",
        "                \"Grad-CAM heatmap\": heatmap_img,\n",
        "                \"Overlay\": PIL.Image.blend(inverse_transform(img), heatmap_img, alpha=0.3),\n",
        "                \"Ground-truth mask\": mask,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "example_gradcam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEJizZVoXX-J"
      },
      "source": [
        "##Functionality tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxNnvz4GafzZ"
      },
      "outputs": [],
      "source": [
        "def example_gradcam2(model: nn.Module, target_layers: Iterable[nn.Module],targets: Iterable[int], batch_size: int = 1,):\n",
        "    grad_cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "    loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    images, labels, masks = next(iter(loader))\n",
        "    images = images.to(device)\n",
        "    heatmaps = grad_cam(images, targets)\n",
        "\n",
        "    for cam, img, mask in zip(heatmaps, images, masks):\n",
        "        heatmap_img = heatmap_to_rgb_image(cam, 0, 1)\n",
        "\n",
        "        show_image_row(\n",
        "            {\n",
        "                \"Input image\": inverse_transform(img),\n",
        "                \"Grad-CAM heatmap\": heatmap_img,\n",
        "                \"Overlay\": PIL.Image.blend(inverse_transform(img), heatmap_img, alpha=0.3),\n",
        "                \"Ground-truth mask\": mask,\n",
        "            }\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhiqJKBraDQI"
      },
      "source": [
        "###1. No target labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "TlMuLrbrsoSC",
        "outputId": "c75d2c3c-1622-4ebf-bc58-190e27f97c7a"
      },
      "outputs": [],
      "source": [
        "example_gradcam2(model=model, target_layers=target_layers, targets=None, batch_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTGvgbVMaLYg"
      },
      "source": [
        "###2. Multiple target layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "qieQ1pw5szkK",
        "outputId": "eece32b4-d7b6-4986-86d0-3df14263dbad"
      },
      "outputs": [],
      "source": [
        "target_layers = [model.layer3[-1]]\n",
        "example_gradcam2(model=model, target_layers=target_layers, targets=None, batch_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMvs3WmNuDb6"
      },
      "source": [
        "###3. Multiple classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "e2Z_L28JuPhL",
        "outputId": "24382fe6-b784-4fdb-bafd-b57824e2f1bd"
      },
      "outputs": [],
      "source": [
        "targets = [3, 4] #Star and Diamond\n",
        "example_gradcam2(model=model, target_layers=target_layers, targets=targets, batch_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcUzgMwbsHdy"
      },
      "source": [
        "# 2. Segment Anything Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjGHh7NlDhEI"
      },
      "source": [
        "\n",
        "## Basic usage [do not modify]\n",
        "The checkpoint takes 360 MB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__5fXL7B0Xz8",
        "outputId": "3c375388-1511-46be-8fcb-63b03dfdff6d"
      },
      "outputs": [],
      "source": [
        "%pip install segment-anything\n",
        "!wget -nc -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtgf-gDJsHdy"
      },
      "outputs": [],
      "source": [
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "sam_checkpoint_path = Path(\"./sam_vit_b_01ec64.pth\")\n",
        "assert sam_checkpoint_path.exists(), \"SAM checkpoint not found.\"\n",
        "\n",
        "# We'll use a single global SAM model to avoid reloading it to memory multiple times.\n",
        "sam_model = sam_model_registry[\"vit_b\"](checkpoint=sam_checkpoint_path)\n",
        "sam_model.to(device)\n",
        "sam_predictor = SamPredictor(sam_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju-OgJclsHdy"
      },
      "outputs": [],
      "source": [
        "class BasicSamPipeline:\n",
        "    def __call__(self, images: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Input: normalized images, shape (B, C=3, H, W).\n",
        "        Output: masks tensor of shape (B, H, W), dtype=bool.\n",
        "        \"\"\"\n",
        "        B, C, H, W = images.shape\n",
        "        # The basic pipeline always uses a single center point for each image.\n",
        "        point_coords = np.array([[(W // 2, H // 2)] for _ in range(B)])\n",
        "\n",
        "        # The basic pipeline always uses a single foreground point, no background points.\n",
        "        point_labels = np.array([[1] for _ in range(B)], dtype=np.int64)\n",
        "\n",
        "        return self.segment(images, point_coords, point_labels)\n",
        "\n",
        "    def segment(\n",
        "        self, images: Tensor, point_coords: np.ndarray, point_labels: np.ndarray\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - images: normalized images, shape (B, C=3, H, W).\n",
        "        - point_coords: point coordinates within each image, shape (B, num_points, 2), format (x,y).\n",
        "            Note the format is not (h,w)=(y,x), but (x,y)!\n",
        "        - point_labels: point labels, shape (B, num_points), dtype int64.\n",
        "            Label 1 is foreground (should be in mask), 0 is background (shouldn't be in mask).\n",
        "\n",
        "        Returns: segmentation masks, shape (B, H, W), dtype=bool.\n",
        "        \"\"\"\n",
        "        B, C, H, W = images.shape\n",
        "        assert C == 3, f\"Expected images.shape=(B, C=3, H, W), got: {images.shape}\"\n",
        "        num_points = point_coords.shape[1]\n",
        "        assert point_coords.shape == (B, num_points, 2), f\"Expected point_coords.shape=({B=}, num_points, 2), got: {point_coords.shape}\"\n",
        "        assert point_labels.shape == (B, num_points), f\"Expected point_labels.shape=({B=}, num_points), got: {point_labels.shape}\"\n",
        "\n",
        "        results = list[Tensor]()\n",
        "        for image, pt_coords, pt_labels in zip(images, point_coords, point_labels, strict=True):\n",
        "            sam_predictor.set_image(np.array(inverse_transform(image)))\n",
        "            masks, scores, _logits = sam_predictor.predict(\n",
        "                point_coords=pt_coords, point_labels=pt_labels, multimask_output=True\n",
        "            )\n",
        "            best_mask = masks[np.argmax(scores)]\n",
        "            results.append(torch.tensor(best_mask, dtype=torch.bool))\n",
        "        return torch.stack(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "riC_HistsHdy",
        "outputId": "ace20676-b6cd-4c84-a934-c83b9477efa7"
      },
      "outputs": [],
      "source": [
        "def example_sam():\n",
        "    indices = [3, 10, 42]\n",
        "    images, labels, gt_masks = next(iter(test_loader))\n",
        "    images, labels, gt_masks = images[indices], labels[indices], gt_masks[indices]\n",
        "\n",
        "    basic_pipeline = BasicSamPipeline()\n",
        "\n",
        "    results = basic_pipeline(images).cpu()\n",
        "\n",
        "    for image, result, gt_mask in zip(images, results, gt_masks, strict=True):\n",
        "        show_image_row(\n",
        "            {\n",
        "                \"Input image\": inverse_transform(image),\n",
        "                \"Basic SAM predicted mask\": result,\n",
        "                \"Ground-truth mask\": gt_mask,\n",
        "            }\n",
        "        )\n",
        "\n",
        "images, labels, gt_masks = next(iter(test_loader))\n",
        "example_sam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js6F1fzJsHdz"
      },
      "source": [
        "## Pipeline implementation and evaluation (add your code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrS-h1gwvyfC"
      },
      "outputs": [],
      "source": [
        "sam_eval_loader = DataLoader(test_dataset, batch_size=5, num_workers=0, pin_memory=use_accel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiKXeBu9ihkq"
      },
      "source": [
        "Abstarct custom pipeline class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UvLWSmdigih"
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List, Tuple # Added this import\n",
        "\n",
        "class CustomPipeline(BasicSamPipeline, ABC):\n",
        "\n",
        "    @abstractmethod\n",
        "    def points(self, images: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Must be implemented in subclasses.\n",
        "        Should return (point_coords, point_labels).\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLZIahV6hF5J"
      },
      "source": [
        "### Evaluation Metrics and Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0jAcl_kShNt"
      },
      "source": [
        "*Metrics per Batch*\n",
        "* Evaluating generated point(s) and reporting the following metrics:\n",
        "    * *hit rate*: how often they fall inside the ground-truth mask;\n",
        "    * *distance*: distance from the center of mass of the ground-truth mask\n",
        "        (the average coordinate of True pixels in the mask).\n",
        "* Evaluating my overall pipeline and reporting the following metric:\n",
        "    * *Intersection over Union (IoU)* of the predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvfLJI5AxmJW"
      },
      "outputs": [],
      "source": [
        "def hit_rate(gt_mask: Tensor, pr_mask: Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Compute hit rate between ground-truth and predicted masks.\n",
        "\n",
        "    Hit rate = TP / (TP + FP)\n",
        "\n",
        "    Where:\n",
        "    - TP = true positives  = predicted foreground that is actually foreground\n",
        "    - FP = false positives = predicted foreground that is actually background\n",
        "\n",
        "    Args:\n",
        "        gt_mask (Tensor): Ground-truth mask of shape (H, W) or (1, H, W).\n",
        "        pr_mask (Tensor): Predicted mask of shape (H, W) or (1, H, W).\n",
        "\n",
        "    Returns:\n",
        "        float: Hit rate in [0, 1]. Returns 0.0 if denominator is zero.\n",
        "    \"\"\"\n",
        "    gt = gt_mask.bool()\n",
        "    pr = pr_mask.bool()\n",
        "\n",
        "    # Squeeze batch/channel dimensions if provided\n",
        "    if gt.ndim == 2:\n",
        "        gt = gt.unsqueeze(0)\n",
        "        pr = pr.unsqueeze(0)\n",
        "\n",
        "    # True positives\n",
        "    hit = (gt & pr).sum(dim=(1,2))\n",
        "\n",
        "    # False positives\n",
        "    miss = (~gt & pr).sum(dim=(1,2))\n",
        "\n",
        "    denom = hit + miss\n",
        "    return torch.mean(hit / (denom)).item()\n",
        "\n",
        "\n",
        "def distance(gt_masks: Tensor, pr_masks: Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Compute average Euclidean distance from predicted foreground pixels\n",
        "    to the centroid of the ground-truth foreground region.\n",
        "\n",
        "    Useful to measure spatial deviation of predicted masks.\n",
        "\n",
        "    Args:\n",
        "        gt_mask (Tensor): Ground truth mask of shape (H, W) or (1, H, W).\n",
        "        pr_mask (Tensor): Predicted mask of shape (H, W) or (1, H, W).\n",
        "\n",
        "    Returns:\n",
        "        float: Mean distance. Returns NaN if either mask has no foreground pixels.\n",
        "    \"\"\"\n",
        "    gt = gt_masks.bool()\n",
        "    pr = pr_masks.bool()\n",
        "\n",
        "    # Squeeze batch/channel dimensions if provided\n",
        "    if gt.ndim == 2:\n",
        "        gt = gt.unsqueeze(0)\n",
        "        pr = pr.unsqueeze(0)\n",
        "\n",
        "    # Get ground truth foreground coordinates\n",
        "    sum = 0\n",
        "    for g,p in zip(gt,pr):\n",
        "      g_coords = g.nonzero(as_tuple=False).float()\n",
        "\n",
        "      # Compute centroid\n",
        "      centroid = g_coords.mean(dim=0)\n",
        "\n",
        "      # Get predicted foreground coordinates\n",
        "      p_coords = p.nonzero(as_tuple=False).float()\n",
        "\n",
        "      # Compute mean Euclidean distance\n",
        "      sum  += (torch.sqrt(((p_coords - centroid) ** 2).sum(dim=1))).mean(dim=0)\n",
        "    return sum/len(gt)\n",
        "\n",
        "\n",
        "def IoU(gt_masks: Tensor, pr_masks: Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Compute the Intersection-over-Union (IoU) between ground-truth and predicted masks.\n",
        "\n",
        "    IoU = intersection / union\n",
        "\n",
        "    Works for single masks (H, W) or batched masks (B, H, W).\n",
        "\n",
        "    Args:\n",
        "        gt_masks (Tensor): Ground-truth mask of shape (H, W) or (B, H, W).\n",
        "        pr_masks (Tensor): Predicted mask of shape (H, W) or (B, H, W).\n",
        "\n",
        "    Returns:\n",
        "        float: Mean IoU across the batch.\n",
        "    \"\"\"\n",
        "    gt = gt_masks.bool()\n",
        "    pr = pr_masks.bool()\n",
        "\n",
        "    # Expand single image to batch dimension\n",
        "    if gt.ndim == 2:\n",
        "        gt = gt.unsqueeze(0)\n",
        "        pr = pr.unsqueeze(0)\n",
        "\n",
        "    intersections = (gt & pr).sum(dim=(1,2)).float()   # (B,)\n",
        "    unions = (gt | pr).sum(dim=(1,2)).float()          # (B,)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    eps = 1e-6\n",
        "    ious = intersections / (unions + eps)\n",
        "\n",
        "    return ious.mean().item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g4toJ8ceRao"
      },
      "source": [
        "### Testing function:\n",
        "  1. Visualizers for seeing what points pixel has chosen and what was the prediction.\n",
        "  2. Function for testing pipeline's performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvkiuqJkevjT"
      },
      "outputs": [],
      "source": [
        "def get_hints_mask(\n",
        "    point_cords: np.ndarray,\n",
        "    point_labels: np.ndarray,\n",
        "    H: int,\n",
        "    W: int,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Create a color-coded visualization mask for SAM point prompts.\n",
        "\n",
        "    Foreground points (label = 1) are shown in red,\n",
        "    background points (label = 0) are shown in blue.\n",
        "\n",
        "    Args:\n",
        "        point_cords (np.ndarray):\n",
        "            Array of point coordinates with shape (N, 2) as (x, y).\n",
        "        point_labels (np.ndarray):\n",
        "            Array of point labels with shape (N,).\n",
        "        H (int):\n",
        "            Height of the output mask.\n",
        "        W (int):\n",
        "            Width of the output mask.\n",
        "\n",
        "    Returns:\n",
        "        Tensor:\n",
        "            RGB mask tensor of shape (H, W, 3).\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize empty RGB mask\n",
        "    mask = torch.zeros((H, W, 3), dtype=torch.float32, device=images.device)\n",
        "\n",
        "    # Mark points according to their label\n",
        "    for cord, label in zip(point_cords, point_labels):\n",
        "        x, y = cord\n",
        "        if label == 1:\n",
        "            mask[y, x, 0] = 1.0  # foreground â†’ red\n",
        "        else:\n",
        "            mask[y, x, 2] = 1.0  # background â†’ blue\n",
        "\n",
        "    return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAHme4SSh7Ay"
      },
      "outputs": [],
      "source": [
        "def visual_test(pipeline: CustomPipeline):\n",
        "    \"\"\"\n",
        "    Visually evaluate a SAM-based segmentation pipeline on selected test samples.\n",
        "\n",
        "    Args:\n",
        "        pipeline (CustomPipeline):\n",
        "            A SAM-based segmentation pipeline providing both `__call__`\n",
        "            and `points` methods.\n",
        "    \"\"\"\n",
        "\n",
        "    # =====================\n",
        "    #  SAMPLE SELECTION\n",
        "    # =====================\n",
        "    indices = [3, 10, 42]\n",
        "\n",
        "    # Fetch a single batch from the test loader\n",
        "    images, labels, gt_masks = next(iter(test_loader))\n",
        "\n",
        "    # Select only the samples of interest\n",
        "    images = images[indices]\n",
        "    labels = labels[indices]\n",
        "    gt_masks = gt_masks[indices]\n",
        "\n",
        "    # =====================\n",
        "    #  PIPELINE EXECUTION\n",
        "    # =====================\n",
        "\n",
        "    # Move images to the correct device before inference\n",
        "    images = images.to(device)\n",
        "\n",
        "    # Run segmentation pipeline\n",
        "    results = pipeline(images)\n",
        "\n",
        "    # Extract point prompts used by the pipeline\n",
        "    point_cords, point_labels = pipeline.points(images)\n",
        "\n",
        "    # =====================\n",
        "    #  VISUALIZATION\n",
        "    # =====================\n",
        "    for i in range(len(indices)):\n",
        "        # Create a visualization mask of pipeline prompts\n",
        "        pipeline_hints = get_hints_mask(\n",
        "            point_cords[i],\n",
        "            point_labels[i],\n",
        "            images[i].shape[1],\n",
        "            images[i].shape[2],\n",
        "        )\n",
        "\n",
        "        # Print per-sample evaluation metrics\n",
        "        print(f\"Score: {hit_rate(gt_masks[i], results[i])}\")\n",
        "        print(f\"Distance: {distance(gt_masks[i], results[i])}\")\n",
        "        print(f\"IoU: {IoU(gt_masks[i], results[i])}\")\n",
        "\n",
        "        # Display input image, prompts, prediction, and ground truth\n",
        "        show_image_row(\n",
        "            {\n",
        "                \"Input image\": inverse_transform(images[i]),\n",
        "                \"Pipeline hints\": pipeline_hints,\n",
        "                \"Basic SAM predicted mask\": results[i],\n",
        "                \"Ground-truth mask\": gt_masks[i],\n",
        "            }\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P5NM6gMTZ8-"
      },
      "outputs": [],
      "source": [
        "def pipeline_test(pipeline: CustomPipeline):\n",
        "    \"\"\"\n",
        "    Evaluate a SAM-based segmentation pipeline on the evaluation dataset.\n",
        "    Batch-level metrics are printed during evaluation, followed by\n",
        "    dataset-level averages.\n",
        "\n",
        "    Metrics computed:\n",
        "    - IoU (Intersection over Union)\n",
        "    - Hit rate (pixel-wise or object-wise coverage metric)\n",
        "    - Distance (spatial discrepancy between prediction and ground truth)\n",
        "\n",
        "    Args:\n",
        "        pipeline (CustomPipeline):\n",
        "            A segmentation pipeline implementing the `__call__` method\n",
        "            and returning boolean segmentation masks.\n",
        "    \"\"\"\n",
        "\n",
        "    # =====================\n",
        "    #  GLOBAL ACCUMULATORS\n",
        "    # =====================\n",
        "    total_Iou = 0.0\n",
        "    total_hit_rate = 0.0\n",
        "    total_distance = 0.0\n",
        "\n",
        "    # Number of evaluation batches\n",
        "    B = len(sam_eval_loader)\n",
        "\n",
        "    # =====================\n",
        "    #  BATCH EVALUATION\n",
        "    # =====================\n",
        "    for index, (images, labels, gt_masks) in enumerate(sam_eval_loader):\n",
        "        # Move images to the target device\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Convert ground-truth masks to boolean\n",
        "        gt = gt_masks.bool()\n",
        "\n",
        "        # Run segmentation pipeline and binarize predictions\n",
        "        pr = pipeline(images).bool()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        total_Iou += IoU(gt, pr)\n",
        "        total_hit_rate += hit_rate(gt, pr)\n",
        "        total_distance += distance(gt, pr)\n",
        "\n",
        "        # Print batch-level metrics\n",
        "        print(\n",
        "            f\"Batch {index + 1} | \"\n",
        "            f\"IoU(batch): {IoU(gt, pr):.3f} | \"\n",
        "            f\"Hit(batch): {hit_rate(gt, pr):.3f} | \"\n",
        "            f\"Distance(batch): {distance(gt, pr):.3f}\"\n",
        "        )\n",
        "\n",
        "    # =====================\n",
        "    #  FINAL DATASET METRICS\n",
        "    # =====================\n",
        "    final_hit_rate = total_hit_rate / B\n",
        "    final_iou = total_Iou / B\n",
        "    final_distance = total_distance / B\n",
        "\n",
        "    print(\"\\nFINAL DATASET METRICS\")\n",
        "    print(f\"Hit rate: {final_hit_rate:.3f}\")\n",
        "    print(f\"IoU: {final_iou:.3f}\")\n",
        "    print(f\"Distance: {final_distance:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNJ2nKHmSsH6"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U4ztfUCI2A2"
      },
      "source": [
        "**Grad-Cam heatmap transofrmation function**: Enhances a Grad-CAM heatmap by spatially propagating activation values to neighboring pixels using an inverse-distance weighting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbO_a0n8iB7o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "\n",
        "def heatmap_transform(heatmap: np.ndarray, K: int = 7) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Enhance a Grad-CAM heatmap by spatially propagating activation values\n",
        "    to neighboring pixels using an inverse-distance weighting.\n",
        "\n",
        "    This transformation:\n",
        "    - Builds a (2K+1) x (2K+1) kernel where weights are proportional to 1 / distance.\n",
        "    - Convolves the heatmap with this kernel.\n",
        "    - Adds the result back to the original heatmap.\n",
        "\n",
        "    Args:\n",
        "        heatmap (np.ndarray):\n",
        "            Input heatmap of shape (H, W).\n",
        "        K (int, optional):\n",
        "            Radius of the neighborhood used for spatial propagation.\n",
        "            Kernel size will be (2K+1) x (2K+1).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray:\n",
        "            Transformed heatmap of shape (H, W).\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a grid of offsets centered at (0, 0)\n",
        "    y, x = np.mgrid[-K : K + 1, -K : K + 1]\n",
        "\n",
        "    # Euclidean distance from the kernel center\n",
        "    dist = np.sqrt(x**2 + y**2)\n",
        "\n",
        "    # Avoid division by zero at the kernel center\n",
        "    dist[K, K] = np.inf\n",
        "\n",
        "    # Inverse-distance weighting kernel\n",
        "    kernel = 1.0 / dist\n",
        "\n",
        "    # Convolve heatmap with the kernel and add to original\n",
        "    return heatmap + convolve2d(\n",
        "        heatmap,\n",
        "        kernel,\n",
        "        mode=\"same\",\n",
        "        boundary=\"fill\",\n",
        "        fillvalue=0,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fame3V4SwVC"
      },
      "source": [
        "**OnlyForegoundSamPipeline:**\n",
        "1. Computes Grad-CAM heatmaps.\n",
        "2. Applies `heatmap_transform` to smooth and expand activations.\n",
        "3. Selects the top-K most activated pixels as foreground points.\n",
        "4. Labels all points as foreground (label = 1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhSg1FvU2-2K"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import override\n",
        "\n",
        "\n",
        "class OnlyForegroundSamPipeline(CustomPipeline):\n",
        "    \"\"\"\n",
        "    SAM prompt-generation pipeline that uses only foreground points\n",
        "    derived from transformed Grad-CAM heatmaps.\n",
        "\n",
        "    The pipeline:\n",
        "    1. Computes Grad-CAM heatmaps.\n",
        "    2. Applies `heatmap_transform` to smooth and expand activations.\n",
        "    3. Selects the top-K most activated pixels as foreground points.\n",
        "    4. Labels all points as foreground (label = 1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        grad_cam: GradCAM,\n",
        "        num_pixels: int = 20,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the foreground-only SAM pipeline.\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module):\n",
        "                Segmentation model used by the base `CustomPipeline`.\n",
        "            grad_cam (GradCAM):\n",
        "                Grad-CAM object used to generate saliency heatmaps.\n",
        "            num_pixels (int, optional):\n",
        "                Number of foreground points selected per image.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.grad_cam = grad_cam\n",
        "        self.num_pixels = num_pixels\n",
        "\n",
        "    @override\n",
        "    def points(self, images: Tensor) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Generate foreground point prompts for SAM.\n",
        "\n",
        "        Processing steps:\n",
        "        1. Compute Grad-CAM heatmaps.\n",
        "        2. Enhance heatmaps using `heatmap_transform`.\n",
        "        3. Select top-K foreground points.\n",
        "        4. Assign foreground labels (1) to all points.\n",
        "\n",
        "        Args:\n",
        "            images (Tensor):\n",
        "                Input images of shape (B, C, H, W) or (C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, np.ndarray]:\n",
        "                - point_coords: array of shape (B, K, 2) with (x, y) pixel coordinates\n",
        "                - point_labels: array of shape (B, K) filled with ones\n",
        "        \"\"\"\n",
        "\n",
        "        # Ensure batch dimension exists\n",
        "        if images.dim() == 3:\n",
        "            images = images.unsqueeze(0)\n",
        "\n",
        "        B, C, H, W = images.shape\n",
        "\n",
        "        # Compute Grad-CAM heatmaps for the batch\n",
        "        heatmaps = self.grad_cam(images)\n",
        "\n",
        "        # Convert heatmaps to numpy if required\n",
        "        if isinstance(heatmaps, torch.Tensor):\n",
        "            heatmaps = heatmaps.cpu().numpy()\n",
        "\n",
        "        # =====================\n",
        "        #  FOREGROUND POINTS\n",
        "        # =====================\n",
        "        foreground_pixels = []\n",
        "\n",
        "        for heatmap in heatmaps:\n",
        "            # Enhance heatmap by propagating activations to neighbors\n",
        "            new_heatmap = heatmap_transform(heatmap)\n",
        "\n",
        "            # Flatten heatmap for top-K selection\n",
        "            flat = new_heatmap.reshape(-1)\n",
        "\n",
        "            # Select indices of the K strongest activations (unsorted)\n",
        "            idx = np.argpartition(flat, -self.num_pixels)[-self.num_pixels:]\n",
        "\n",
        "            # Sort selected indices by activation strength (descending)\n",
        "            idx = idx[np.argsort(flat[idx])[::-1]]\n",
        "\n",
        "            # Convert flat indices back to (x, y) coordinates\n",
        "            ys = idx // W\n",
        "            xs = idx % W\n",
        "            coords = np.stack([xs, ys], axis=1)  # shape (K, 2)\n",
        "\n",
        "            foreground_pixels.append(coords)\n",
        "\n",
        "        # =====================\n",
        "        #  FORMAT SAM INPUTS\n",
        "        # =====================\n",
        "\n",
        "        # Preallocate arrays for SAM-compatible inputs\n",
        "        point_coords = np.zeros((B, self.num_pixels, 2), dtype=np.int64)\n",
        "\n",
        "        # All points are foreground â†’ label = 1\n",
        "        point_labels = np.ones((B, self.num_pixels), dtype=np.int64)\n",
        "\n",
        "        # Populate coordinate array\n",
        "        for i, coords in enumerate(foreground_pixels):\n",
        "            point_coords[i] = coords\n",
        "\n",
        "        return point_coords, point_labels\n",
        "\n",
        "    @override\n",
        "    def __call__(self, images: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Run segmentation using foreground-only point prompts.\n",
        "\n",
        "        Args:\n",
        "            images (Tensor):\n",
        "                Input images of shape (B, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            Tensor:\n",
        "                Segmentation output produced by the base `segment` method.\n",
        "        \"\"\"\n",
        "        point_coords, point_labels = self.points(images)\n",
        "        return self.segment(images, point_coords, point_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R9piJRR_AZk"
      },
      "outputs": [],
      "source": [
        "if type(model).__name__ == \"ResNet\":\n",
        "    target_layers = [model.layer2[-1]]\n",
        "else:\n",
        "    target_layers = [\n",
        "        model.get_submodule(\"features.2.0\"),\n",
        "        model.get_submodule(\"features.3.0\"),\n",
        "        model.get_submodule(\"features.4.0\"),\n",
        "    ]\n",
        "grad_cam = GradCAM(model=model, target_layers=target_layers)\n",
        "num_pixels = 1\n",
        "pipeline = OnlyForegroundSamPipeline(model=model, grad_cam=grad_cam, num_pixels=num_pixels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f4WaLgRft-Wn"
      },
      "outputs": [],
      "source": [
        "visual_test(pipeline=pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yzc_aL_vxCf"
      },
      "outputs": [],
      "source": [
        "pipeline_test(pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhQEtX6EhZLD"
      },
      "source": [
        "**BackgroundAndForegroundSamPipeline**:\n",
        "1. Uses Grad-CAM heatmaps to select the most salient foreground pixels\n",
        "2. Apply `heatmap_transform` to smooth and expand activations\n",
        "3. Computes the center of foreground points.\n",
        "4. Samples background points sufficiently far from the foreground center.\n",
        "5. Combines both into point coordinates and labels compatible with SAM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yXTU-_ehZLD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from typing import Tuple\n",
        "from typing_extensions import override\n",
        "\n",
        "\n",
        "class BothSamPipeline(CustomPipeline):\n",
        "    \"\"\"\n",
        "    Pipeline that combines Grad-CAMâ€“based foreground point selection\n",
        "    with random background point sampling for SAM-style segmentation.\n",
        "\n",
        "    The pipeline:\n",
        "    1. Uses Grad-CAM heatmaps to select the most salient foreground pixels.\n",
        "    2. Apply `heatmap_transform` to smooth and expand activations.\n",
        "    3. Computes the center of foreground points.\n",
        "    4. Samples background points sufficiently far from the foreground center.\n",
        "    5. Combines both into point coordinates and labels compatible with SAM.\n",
        "\n",
        "    Foreground points are labeled as `1`, background points as `0`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        grad_cam: GradCAM,\n",
        "        num_for_pixels: int = 20,\n",
        "        num_back_pixels: int = 100,\n",
        "        min_distance: float = 50,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline.\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module):\n",
        "                Segmentation model used by the base `CustomPipeline`.\n",
        "            grad_cam (GradCAM):\n",
        "                Grad-CAM object used to compute saliency heatmaps.\n",
        "            num_for_pixels (int, optional):\n",
        "                Number of foreground (positive) points per image.\n",
        "            num_back_pixels (int, optional):\n",
        "                Number of background (negative) points per image.\n",
        "            min_distance (float, optional):\n",
        "                Minimum Euclidean distance (in pixels) between background\n",
        "                points and the foreground center.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.grad_cam = grad_cam\n",
        "        self.num_for_pixels = num_for_pixels\n",
        "        self.num_back_pixels = num_back_pixels\n",
        "        self.min_distance = min_distance\n",
        "\n",
        "    @override\n",
        "    def points(self, images: Tensor) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Generate foreground and background point prompts for SAM.\n",
        "\n",
        "        Args:\n",
        "            images (Tensor):\n",
        "                Input images of shape (B, C, H, W) or (C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, np.ndarray]:\n",
        "                - point_coords: array of shape (B, N, 2) with (x, y) pixel coordinates\n",
        "                - point_labels: array of shape (B, N) with labels\n",
        "                  (1 = foreground, 0 = background)\n",
        "        \"\"\"\n",
        "\n",
        "        # Ensure batch dimension exists\n",
        "        if images.dim() == 3:\n",
        "            images = images.unsqueeze(0)\n",
        "\n",
        "        B, C, H, W = images.shape\n",
        "\n",
        "        # Compute Grad-CAM heatmaps for each image\n",
        "        heatmaps = self.grad_cam(images)\n",
        "\n",
        "        # Convert heatmaps to numpy if needed\n",
        "        if isinstance(heatmaps, torch.Tensor):\n",
        "            heatmaps = heatmaps.cpu().numpy()\n",
        "\n",
        "        # =====================\n",
        "        #  FOREGROUND POINTS\n",
        "        # =====================\n",
        "        foreground_pixels = []\n",
        "        centres = []\n",
        "\n",
        "        for heatmap in heatmaps:\n",
        "            # Apply any required post-processing to the heatmap\n",
        "            new_heatmap = heatmap_transform(heatmap)\n",
        "\n",
        "            # Flatten heatmap to 1D for top-K selection\n",
        "            flat = new_heatmap.reshape(-1)\n",
        "\n",
        "            # Select indices of top-K foreground activations (unsorted)\n",
        "            idx = np.argpartition(flat, -self.num_for_pixels)[-self.num_for_pixels:]\n",
        "\n",
        "            # Sort selected indices by activation value (descending)\n",
        "            idx = idx[np.argsort(flat[idx])[::-1]]\n",
        "\n",
        "            # Convert flat indices back to (x, y) coordinates\n",
        "            ys = idx // W\n",
        "            xs = idx % W\n",
        "            coords = np.stack([xs, ys], axis=1)  # shape (K, 2)\n",
        "\n",
        "            foreground_pixels.append(coords)\n",
        "\n",
        "            # Compute geometric center of foreground points\n",
        "            centre = coords.mean(axis=0)\n",
        "            centres.append(centre)\n",
        "\n",
        "        # Stack centers for the whole batch\n",
        "        centres = np.stack(centres, axis=0)  # shape (B, 2)\n",
        "\n",
        "        # =====================\n",
        "        #  BACKGROUND POINTS\n",
        "        # =====================\n",
        "        background_pixels = []\n",
        "\n",
        "        for i in range(B):\n",
        "            centre = centres[i]\n",
        "\n",
        "            # Random permutation of all pixel indices\n",
        "            perm = np.random.permutation(H * W)\n",
        "            ys = perm // W\n",
        "            xs = perm % W\n",
        "            coords = np.stack([xs, ys], axis=1)\n",
        "\n",
        "            # Compute Euclidean distance from foreground center\n",
        "            dists = np.sqrt(((coords - centre) ** 2).sum(axis=1))\n",
        "\n",
        "            # Keep only points far enough from the foreground\n",
        "            eligible = coords[dists > self.min_distance]\n",
        "\n",
        "            # Ensure the required number of background points\n",
        "            if len(eligible) < self.num_back_pixels:\n",
        "                # Sample with replacement if insufficient points\n",
        "                pad_idx = np.random.choice(len(eligible), self.num_back_pixels, replace=True)\n",
        "                eligible = eligible[pad_idx]\n",
        "            else:\n",
        "                eligible = eligible[:self.num_back_pixels]\n",
        "\n",
        "            background_pixels.append(eligible)\n",
        "\n",
        "        # =====================\n",
        "        #  COMBINE INTO SAM INPUT\n",
        "        # =====================\n",
        "        total_points = self.num_for_pixels + self.num_back_pixels\n",
        "\n",
        "        # Allocate arrays for SAM-compatible inputs\n",
        "        point_coords = np.zeros((B, total_points, 2), dtype=np.int64)\n",
        "        point_labels = np.zeros((B, total_points), dtype=np.int64)\n",
        "\n",
        "        for i, (f_coords, b_coords) in enumerate(zip(foreground_pixels, background_pixels)):\n",
        "            # Foreground points (label = 1)\n",
        "            point_coords[i, :self.num_for_pixels] = f_coords\n",
        "            point_labels[i, :self.num_for_pixels] = 1\n",
        "\n",
        "            # Background points (label = 0)\n",
        "            point_coords[i, self.num_for_pixels:] = b_coords\n",
        "            point_labels[i, self.num_for_pixels:] = 0\n",
        "\n",
        "        return point_coords, point_labels\n",
        "\n",
        "    @override\n",
        "    def __call__(self, images: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Run the full segmentation pipeline.\n",
        "\n",
        "        Args:\n",
        "            images (Tensor):\n",
        "                Input images of shape (B, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            Tensor:\n",
        "                Segmentation output produced by the base `segment` method.\n",
        "        \"\"\"\n",
        "        point_coords, point_labels = self.points(images)\n",
        "        return self.segment(images, point_coords, point_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vPAQHWJhZLD"
      },
      "outputs": [],
      "source": [
        "if type(model).__name__ == \"ResNet\":\n",
        "    target_layers = [model.layer2[-1]]\n",
        "else:\n",
        "    target_layers = [\n",
        "        model.get_submodule(\"features.2.0\"),\n",
        "        model.get_submodule(\"features.3.0\"),\n",
        "        model.get_submodule(\"features.4.0\"),\n",
        "    ]\n",
        "grad_cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "num_for_pixels = 10\n",
        "num_back_pixels = 3\n",
        "min_distance = 1\n",
        "\n",
        "\n",
        "pipeline = BothSamPipeline(model=model, grad_cam=grad_cam, num_back_pixels=num_back_pixels, num_for_pixels=num_for_pixels, min_distance=min_distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U18_vQrw_H0U"
      },
      "outputs": [],
      "source": [
        "visual_test(pipeline=pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8Nl4b-FhZLD"
      },
      "outputs": [],
      "source": [
        "pipeline_test(pipeline)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vMwHjowPDYBj",
        "uXa0LsWEW2SI",
        "2ZPb1h56W2SI"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
